conceptLinks:
  Dockerfile Basics: "#lesson-dockerfile"
  Docker Commands: "#lesson-build-run-stop"
  kubectl Basics: "#lesson-your-first-pod"
  Pod YAML: "#lesson-your-first-pod"
  Dockerfile Optimization: "#lesson-image-layers"
  Pod Manifest Writing: "#lesson-your-first-pod"
  kubectl Troubleshooting: "#lesson-verify-your-cluster"

variants:
  warmups:
    - id: warmup_1
      concept: Dockerfile Basics
      variants:
        - id: v1
          title: Node.js Dockerfile
          description: Write a Dockerfile for a Node.js application that uses <code>node:20-alpine</code> as the base image, sets the working directory to <code>/app</code>, copies <code>package*.json</code>, runs <code>npm install</code>, copies the rest of the code, exposes port 3000, and starts with <code>node server.js</code>.
          hints:
            - "Start with <code>FROM node:20-alpine</code>"
            - "Use <code>WORKDIR /app</code> to set the working directory"
            - "Copy dependency files first, install, then copy the rest for better caching"
          solution: |-
            FROM node:20-alpine
            WORKDIR /app
            COPY package*.json ./
            RUN npm install
            COPY . .
            EXPOSE 3000
            CMD ["node", "server.js"]
        - id: v2
          title: Python Flask Dockerfile
          description: Write a Dockerfile for a Python Flask app. Use <code>python:3.12-slim</code> as base, set workdir to <code>/app</code>, copy and install <code>requirements.txt</code>, copy code, expose port 5000, and run with <code>python app.py</code>.
          hints:
            - "Use <code>FROM python:3.12-slim</code> for a smaller image"
            - "Install dependencies with <code>RUN pip install -r requirements.txt</code>"
          solution: |-
            FROM python:3.12-slim
            WORKDIR /app
            COPY requirements.txt .
            RUN pip install -r requirements.txt
            COPY . .
            EXPOSE 5000
            CMD ["python", "app.py"]
        - id: v3
          title: Go Static Binary Dockerfile
          description: Write a Dockerfile for a Go application. Use <code>golang:1.22-alpine</code> as base, set workdir to <code>/app</code>, copy <code>go.mod</code> and <code>go.sum</code>, run <code>go mod download</code>, copy code, build with <code>go build -o main .</code>, expose port 8080, and run <code>./main</code>.
          hints:
            - "Copy go.mod and go.sum first for dependency caching"
            - "Use <code>RUN go build -o main .</code> to compile"
          solution: |-
            FROM golang:1.22-alpine
            WORKDIR /app
            COPY go.mod go.sum ./
            RUN go mod download
            COPY . .
            RUN go build -o main .
            EXPOSE 8080
            CMD ["./main"]
        - id: v4
          title: Java Spring Boot Dockerfile
          description: Write a Dockerfile for a Java Spring Boot app. Use <code>eclipse-temurin:21-jdk-alpine</code>, set workdir to <code>/app</code>, copy the pre-built JAR file <code>target/app.jar</code>, expose port 8080, and run it with <code>java -jar app.jar</code>.
          hints:
            - "Use <code>COPY target/app.jar app.jar</code> to copy the JAR"
            - "Run with <code>CMD [\"java\", \"-jar\", \"app.jar\"]</code>"
          solution: |-
            FROM eclipse-temurin:21-jdk-alpine
            WORKDIR /app
            COPY target/app.jar app.jar
            EXPOSE 8080
            CMD ["java", "-jar", "app.jar"]
        - id: v5
          title: Fix Broken Dockerfile - Missing WORKDIR
          description: "This Dockerfile fails because files are copied to the wrong location. Fix it by adding the missing <code>WORKDIR</code> instruction:<br><pre>FROM node:20-alpine\nCOPY package.json ./\nRUN npm install\nCOPY . .\nCMD [\"node\", \"server.js\"]</pre>"
          hints:
            - "Add <code>WORKDIR /app</code> after the FROM instruction"
            - "Without WORKDIR, files go to the root directory which can cause conflicts"
          solution: |-
            FROM node:20-alpine
            WORKDIR /app
            COPY package.json ./
            RUN npm install
            COPY . .
            CMD ["node", "server.js"]
        - id: v6
          title: Fix Broken Dockerfile - Wrong Layer Order
          description: "This Dockerfile rebuilds dependencies every time code changes. Fix the layer order for better caching:<br><pre>FROM python:3.12-slim\nWORKDIR /app\nCOPY . .\nRUN pip install -r requirements.txt\nCMD [\"python\", \"app.py\"]</pre>"
          hints:
            - "Copy and install requirements.txt BEFORE copying the rest of the code"
            - "Dependencies change less often than code, so install them first"
          solution: |-
            FROM python:3.12-slim
            WORKDIR /app
            COPY requirements.txt .
            RUN pip install -r requirements.txt
            COPY . .
            CMD ["python", "app.py"]
        - id: v7
          title: Nginx Static Site Dockerfile
          description: Write a Dockerfile that serves a static website. Use <code>nginx:1.25-alpine</code> as base, copy the <code>html/</code> directory to <code>/usr/share/nginx/html</code>, and expose port 80.
          hints:
            - "Use <code>COPY html/ /usr/share/nginx/html</code>"
            - "Nginx starts automatically as the default CMD"
          solution: |-
            FROM nginx:1.25-alpine
            COPY html/ /usr/share/nginx/html
            EXPOSE 80
        - id: v8
          title: Ruby Sinatra Dockerfile
          description: Write a Dockerfile for a Ruby Sinatra app. Use <code>ruby:3.3-alpine</code>, set workdir to <code>/app</code>, copy <code>Gemfile</code> and <code>Gemfile.lock</code>, run <code>bundle install</code>, copy code, expose port 4567, and run <code>ruby app.rb</code>.
          hints:
            - "Copy Gemfile and Gemfile.lock before bundle install for caching"
            - "Sinatra's default port is 4567"
          solution: |-
            FROM ruby:3.3-alpine
            WORKDIR /app
            COPY Gemfile Gemfile.lock ./
            RUN bundle install
            COPY . .
            EXPOSE 4567
            CMD ["ruby", "app.rb"]
        - id: v9
          title: Fix Broken Dockerfile - Missing FROM
          description: "This Dockerfile is missing the base image. Fix it to use <code>node:20-alpine</code>:<br><pre>WORKDIR /app\nCOPY package.json ./\nRUN npm install\nCOPY . .\nCMD [\"node\", \"index.js\"]</pre>"
          hints:
            - "Every Dockerfile must start with a <code>FROM</code> instruction"
          solution: |-
            FROM node:20-alpine
            WORKDIR /app
            COPY package.json ./
            RUN npm install
            COPY . .
            CMD ["node", "index.js"]
        - id: v10
          title: Dockerfile with ENV and ARG
          description: Write a Dockerfile for a Node.js app that sets an environment variable <code>NODE_ENV=production</code> and uses a build argument <code>APP_VERSION</code> as a label. Use <code>node:20-alpine</code>, workdir <code>/app</code>, copy and install deps, copy code, expose 3000.
          hints:
            - "Use <code>ARG APP_VERSION</code> to declare a build argument"
            - "Use <code>ENV NODE_ENV=production</code> to set runtime environment"
            - "Use <code>LABEL version=$APP_VERSION</code> to apply the arg"
          solution: |-
            FROM node:20-alpine
            ARG APP_VERSION
            LABEL version=$APP_VERSION
            ENV NODE_ENV=production
            WORKDIR /app
            COPY package*.json ./
            RUN npm install
            COPY . .
            EXPOSE 3000
            CMD ["node", "server.js"]
        - id: v11
          title: Rust Dockerfile
          description: Write a Dockerfile for a Rust application. Use <code>rust:1.77-alpine</code>, set workdir to <code>/app</code>, copy <code>Cargo.toml</code> and <code>Cargo.lock</code>, copy <code>src/</code>, build with <code>cargo build --release</code>, expose port 8080, and run <code>./target/release/myapp</code>.
          hints:
            - "Copy Cargo files first for dependency caching"
            - "Use <code>--release</code> flag for optimized builds"
          solution: |-
            FROM rust:1.77-alpine
            WORKDIR /app
            COPY Cargo.toml Cargo.lock ./
            COPY src/ src/
            RUN cargo build --release
            EXPOSE 8080
            CMD ["./target/release/myapp"]
        - id: v12
          title: Fix Broken Dockerfile - Wrong CMD Syntax
          description: "This Dockerfile uses shell form for CMD which doesn't handle signals properly. Fix it to use exec form:<br><pre>FROM node:20-alpine\nWORKDIR /app\nCOPY . .\nRUN npm install\nCMD node server.js</pre>"
          hints:
            - "Exec form uses JSON array syntax: <code>CMD [\"node\", \"server.js\"]</code>"
            - "Shell form wraps the command in <code>/bin/sh -c</code> which doesn't forward signals"
          solution: |-
            FROM node:20-alpine
            WORKDIR /app
            COPY package*.json ./
            RUN npm install
            COPY . .
            CMD ["node", "server.js"]

    - id: warmup_2
      concept: Docker Commands
      variants:
        - id: v1
          title: Build an Image
          description: Write the Docker command to build an image tagged <code>myapp:v1</code> from the Dockerfile in the current directory.
          hints:
            - "Use <code>docker build</code> with the <code>-t</code> flag for tagging"
            - "The <code>.</code> at the end specifies the build context"
          solution: |-
            docker build -t myapp:v1 .
        - id: v2
          title: Run a Container in Background
          description: Write the Docker command to run a container named <code>web</code> from image <code>nginx:1.25</code> in detached mode, mapping host port 8080 to container port 80.
          hints:
            - "Use <code>-d</code> for detached mode"
            - "Use <code>-p 8080:80</code> for port mapping (host:container)"
            - "Use <code>--name web</code> to name the container"
          solution: |-
            docker run -d -p 8080:80 --name web nginx:1.25
        - id: v3
          title: View Container Logs
          description: Write the Docker command to follow (stream) the logs of a running container named <code>myapp</code>.
          hints:
            - "Use <code>docker logs</code> with the <code>-f</code> flag to follow"
          solution: |-
            docker logs -f myapp
        - id: v4
          title: Shell into a Running Container
          description: Write the Docker command to open an interactive shell (<code>/bin/sh</code>) inside a running container named <code>myapp</code>.
          hints:
            - "Use <code>docker exec</code> with <code>-it</code> flags"
            - "<code>-i</code> is interactive, <code>-t</code> allocates a pseudo-TTY"
          solution: |-
            docker exec -it myapp /bin/sh
        - id: v5
          title: List Running Containers
          description: Write the Docker command to list all containers, including stopped ones.
          hints:
            - "Use <code>docker ps</code> with the <code>-a</code> flag"
          solution: |-
            docker ps -a
        - id: v6
          title: Stop and Remove a Container
          description: Write the Docker commands to stop a container named <code>web</code> and then remove it.
          hints:
            - "Use <code>docker stop</code> followed by <code>docker rm</code>"
            - "You can chain them with <code>&&</code>"
          solution: |-
            docker stop web && docker rm web
        - id: v7
          title: Run with Environment Variables
          description: Write the Docker command to run a container from <code>postgres:16</code> named <code>db</code> in detached mode, setting environment variable <code>POSTGRES_PASSWORD=secret</code> and mapping port 5432.
          hints:
            - "Use <code>-e</code> to set environment variables"
            - "Map port 5432:5432 for PostgreSQL"
          solution: |-
            docker run -d --name db -e POSTGRES_PASSWORD=secret -p 5432:5432 postgres:16
        - id: v8
          title: Remove All Stopped Containers
          description: Write the Docker command to remove all stopped containers at once.
          hints:
            - "Use <code>docker container prune</code>"
            - "Or use <code>docker rm $(docker ps -aq -f status=exited)</code>"
          solution: |-
            docker container prune -f
        - id: v9
          title: Build with No Cache
          description: Write the Docker command to build an image tagged <code>myapp:v2</code> without using any cached layers.
          hints:
            - "Use the <code>--no-cache</code> flag with docker build"
          solution: |-
            docker build --no-cache -t myapp:v2 .
        - id: v10
          title: Run with Volume Mount
          description: Write the Docker command to run an <code>nginx:1.25</code> container named <code>web</code> in detached mode, mounting the host directory <code>./html</code> to <code>/usr/share/nginx/html</code> in the container, and mapping port 8080 to 80.
          hints:
            - "Use <code>-v</code> for volume mounting with the syntax <code>host_path:container_path</code>"
          solution: |-
            docker run -d --name web -v ./html:/usr/share/nginx/html -p 8080:80 nginx:1.25
        - id: v11
          title: Inspect a Container
          description: Write the Docker command to see the full details (inspect) of a container named <code>web</code>, and then a second command to extract just its IP address.
          hints:
            - "Use <code>docker inspect</code>"
            - "Use <code>--format</code> or <code>-f</code> with a Go template to extract fields"
          solution: |-
            docker inspect web
            docker inspect -f '{{range.NetworkSettings.Networks}}{{.IPAddress}}{{end}}' web
        - id: v12
          title: Tag and Push an Image
          description: Write the Docker commands to tag an existing local image <code>myapp:v1</code> for Docker Hub as <code>myuser/myapp:v1</code> and then push it.
          hints:
            - "Use <code>docker tag</code> to create the new tag"
            - "Use <code>docker push</code> to upload"
          solution: |-
            docker tag myapp:v1 myuser/myapp:v1
            docker push myuser/myapp:v1

    - id: warmup_3
      concept: kubectl Basics
      variants:
        - id: v1
          title: Run a Pod Imperatively
          description: Write the kubectl command to create a Pod named <code>nginx</code> using the <code>nginx:1.25</code> image.
          hints:
            - "Use <code>kubectl run</code> with the <code>--image</code> flag"
          solution: |-
            kubectl run nginx --image=nginx:1.25
        - id: v2
          title: List All Pods
          description: Write the kubectl command to list all Pods in the default namespace with additional details (IP, node).
          hints:
            - "Use <code>kubectl get pods</code>"
            - "Add <code>-o wide</code> for extra columns"
          solution: |-
            kubectl get pods -o wide
        - id: v3
          title: Describe a Pod
          description: Write the kubectl command to see detailed information about a Pod named <code>nginx</code>, including events and conditions.
          hints:
            - "Use <code>kubectl describe pod</code>"
          solution: |-
            kubectl describe pod nginx
        - id: v4
          title: View Pod Logs
          description: Write the kubectl command to view the logs of a Pod named <code>myapp</code> and follow them in real time.
          hints:
            - "Use <code>kubectl logs</code> with the <code>-f</code> flag to follow"
          solution: |-
            kubectl logs -f myapp
        - id: v5
          title: Delete a Pod
          description: Write the kubectl command to delete a Pod named <code>nginx</code>.
          hints:
            - "Use <code>kubectl delete pod</code>"
          solution: |-
            kubectl delete pod nginx
        - id: v6
          title: Port-Forward to a Pod
          description: Write the kubectl command to forward local port 8080 to port 80 on a Pod named <code>nginx</code>.
          hints:
            - "Use <code>kubectl port-forward</code> with syntax <code>pod/name local:remote</code>"
          solution: |-
            kubectl port-forward pod/nginx 8080:80
        - id: v7
          title: Apply a YAML File
          description: Write the kubectl command to create or update resources defined in a file named <code>pod.yaml</code>.
          hints:
            - "Use <code>kubectl apply -f</code> for declarative management"
          solution: |-
            kubectl apply -f pod.yaml
        - id: v8
          title: Get Pods in All Namespaces
          description: Write the kubectl command to list Pods across all namespaces.
          hints:
            - "Use the <code>--all-namespaces</code> or <code>-A</code> flag"
          solution: |-
            kubectl get pods --all-namespaces
        - id: v9
          title: Exec into a Pod
          description: Write the kubectl command to open an interactive shell (<code>/bin/sh</code>) inside a running Pod named <code>myapp</code>.
          hints:
            - "Use <code>kubectl exec</code> with <code>-it</code> flags"
            - "Separate the command with <code>--</code>"
          solution: |-
            kubectl exec -it myapp -- /bin/sh
        - id: v10
          title: Get Pods with Labels
          description: Write the kubectl command to list all Pods that have the label <code>app=nginx</code>.
          hints:
            - "Use the <code>-l</code> or <code>--selector</code> flag"
          solution: |-
            kubectl get pods -l app=nginx
        - id: v11
          title: Delete Resources from a File
          description: Write the kubectl command to delete all resources defined in <code>pod.yaml</code>.
          hints:
            - "Use <code>kubectl delete -f</code>"
          solution: |-
            kubectl delete -f pod.yaml
        - id: v12
          title: Check Cluster Info
          description: Write the kubectl commands to check the cluster information and list all nodes.
          hints:
            - "Use <code>kubectl cluster-info</code> and <code>kubectl get nodes</code>"
          solution: |-
            kubectl cluster-info
            kubectl get nodes
        - id: v13
          title: Get Pod YAML Output
          description: Write the kubectl command to get the full YAML definition of a running Pod named <code>nginx</code>.
          hints:
            - "Use <code>-o yaml</code> to output in YAML format"
          solution: |-
            kubectl get pod nginx -o yaml

    - id: warmup_4
      concept: Pod YAML
      variants:
        - id: v1
          title: Basic Nginx Pod
          description: "Write a Pod manifest for an nginx Pod named <code>nginx</code> with image <code>nginx:1.25</code> exposing container port 80 and labeled <code>app: nginx</code>."
          hints:
            - "apiVersion is <code>v1</code> and kind is <code>Pod</code>"
            - "Container port goes under <code>spec.containers[].ports[].containerPort</code>"
          solution: |-
            apiVersion: v1
            kind: Pod
            metadata:
              name: nginx
              labels:
                app: nginx
            spec:
              containers:
              - name: nginx
                image: nginx:1.25
                ports:
                - containerPort: 80
        - id: v2
          title: Redis Pod
          description: "Write a Pod manifest for a Redis cache named <code>redis</code> with image <code>redis:7-alpine</code>, exposing port 6379, with labels <code>app: redis</code> and <code>tier: cache</code>."
          hints:
            - "Multiple labels go under <code>metadata.labels</code>"
            - "Redis default port is 6379"
          solution: |-
            apiVersion: v1
            kind: Pod
            metadata:
              name: redis
              labels:
                app: redis
                tier: cache
            spec:
              containers:
              - name: redis
                image: redis:7-alpine
                ports:
                - containerPort: 6379
        - id: v3
          title: Pod with Custom Command
          description: "Write a Pod manifest named <code>busybox</code> using image <code>busybox:1.36</code> that runs the command <code>echo \"Hello Kubernetes!\"</code> and then sleeps for 3600 seconds."
          hints:
            - "Use <code>command</code> and <code>args</code> in the container spec"
            - "Use <code>/bin/sh -c</code> to run shell commands"
          solution: |-
            apiVersion: v1
            kind: Pod
            metadata:
              name: busybox
            spec:
              containers:
              - name: busybox
                image: busybox:1.36
                command: ["/bin/sh", "-c"]
                args: ["echo 'Hello Kubernetes!' && sleep 3600"]
        - id: v4
          title: Pod with Environment Variables
          description: "Write a Pod manifest named <code>myapp</code> using image <code>myapp:v1</code> with environment variables <code>DB_HOST=postgres</code> and <code>DB_PORT=5432</code>, exposing port 8080."
          hints:
            - "Environment variables go under <code>spec.containers[].env</code>"
            - "Each env entry has <code>name</code> and <code>value</code>"
          solution: |-
            apiVersion: v1
            kind: Pod
            metadata:
              name: myapp
              labels:
                app: myapp
            spec:
              containers:
              - name: myapp
                image: myapp:v1
                ports:
                - containerPort: 8080
                env:
                - name: DB_HOST
                  value: "postgres"
                - name: DB_PORT
                  value: "5432"
        - id: v5
          title: PostgreSQL Pod
          description: "Write a Pod manifest named <code>postgres</code> using image <code>postgres:16</code>, exposing port 5432, with environment variable <code>POSTGRES_PASSWORD=mysecret</code> and labels <code>app: postgres</code> and <code>tier: database</code>."
          hints:
            - "PostgreSQL requires the POSTGRES_PASSWORD environment variable"
            - "Default PostgreSQL port is 5432"
          solution: |-
            apiVersion: v1
            kind: Pod
            metadata:
              name: postgres
              labels:
                app: postgres
                tier: database
            spec:
              containers:
              - name: postgres
                image: postgres:16
                ports:
                - containerPort: 5432
                env:
                - name: POSTGRES_PASSWORD
                  value: "mysecret"
        - id: v6
          title: Pod with Resource Limits
          description: "Write a Pod manifest named <code>api</code> using image <code>api:v1</code> with resource requests of 128Mi memory and 250m CPU, and limits of 256Mi memory and 500m CPU."
          hints:
            - "Resources go under <code>spec.containers[].resources</code>"
            - "Use <code>requests</code> for guaranteed resources and <code>limits</code> for maximum"
          solution: |-
            apiVersion: v1
            kind: Pod
            metadata:
              name: api
              labels:
                app: api
            spec:
              containers:
              - name: api
                image: api:v1
                resources:
                  requests:
                    memory: "128Mi"
                    cpu: "250m"
                  limits:
                    memory: "256Mi"
                    cpu: "500m"
        - id: v7
          title: Pod with Named Port
          description: "Write a Pod manifest named <code>web</code> using image <code>httpd:2.4</code> with a container port 80 named <code>http</code> and labels <code>app: web</code>."
          hints:
            - "Ports can have a <code>name</code> field for reference by Services"
          solution: |-
            apiVersion: v1
            kind: Pod
            metadata:
              name: web
              labels:
                app: web
            spec:
              containers:
              - name: web
                image: httpd:2.4
                ports:
                - name: http
                  containerPort: 80
        - id: v8
          title: Multi-Container Pod
          description: "Write a Pod manifest named <code>webapp</code> with two containers. The first is <code>app</code> using <code>nginx:1.25</code> on port 80. The second is <code>sidecar</code> using <code>busybox:1.36</code> that runs <code>while true; do echo heartbeat; sleep 30; done</code>."
          hints:
            - "Multiple containers go in the <code>spec.containers</code> array"
            - "Each container needs its own <code>name</code> and <code>image</code>"
          solution: |-
            apiVersion: v1
            kind: Pod
            metadata:
              name: webapp
              labels:
                app: webapp
            spec:
              containers:
              - name: app
                image: nginx:1.25
                ports:
                - containerPort: 80
              - name: sidecar
                image: busybox:1.36
                command: ["/bin/sh", "-c"]
                args: ["while true; do echo heartbeat; sleep 30; done"]
        - id: v9
          title: Pod with Restart Policy
          description: "Write a Pod manifest named <code>batch-job</code> using image <code>python:3.12-slim</code> that runs <code>python script.py</code> with restart policy <code>Never</code> (for batch/one-shot work)."
          hints:
            - "Use <code>spec.restartPolicy</code> set to <code>Never</code>"
            - "Default restartPolicy is <code>Always</code>"
          solution: |-
            apiVersion: v1
            kind: Pod
            metadata:
              name: batch-job
            spec:
              restartPolicy: Never
              containers:
              - name: batch-job
                image: python:3.12-slim
                command: ["python", "script.py"]
        - id: v10
          title: Pod with Multiple Labels and Annotations
          description: "Write a Pod manifest named <code>frontend</code> using image <code>nginx:1.25</code> with labels <code>app: frontend</code>, <code>env: production</code>, <code>team: platform</code> and annotation <code>description: \"Main frontend pod\"</code>."
          hints:
            - "Annotations go under <code>metadata.annotations</code>"
            - "Annotations are for non-identifying metadata"
          solution: |-
            apiVersion: v1
            kind: Pod
            metadata:
              name: frontend
              labels:
                app: frontend
                env: production
                team: platform
              annotations:
                description: "Main frontend pod"
            spec:
              containers:
              - name: frontend
                image: nginx:1.25
                ports:
                - containerPort: 80
        - id: v11
          title: Pod with Image Pull Policy
          description: "Write a Pod manifest named <code>dev-app</code> using image <code>myapp:latest</code> with <code>imagePullPolicy: Always</code> to ensure the latest image is always pulled."
          hints:
            - "Use <code>imagePullPolicy</code> in the container spec"
            - "Options are <code>Always</code>, <code>IfNotPresent</code>, and <code>Never</code>"
          solution: |-
            apiVersion: v1
            kind: Pod
            metadata:
              name: dev-app
              labels:
                app: dev-app
            spec:
              containers:
              - name: dev-app
                image: myapp:latest
                imagePullPolicy: Always
                ports:
                - containerPort: 8080

  challenges:
    - id: challenge_1
      block: 1
      difficulty: 1
      concept: Dockerfile Optimization
      variants:
        - id: v1
          title: Multi-Stage Node.js Build
          description: "Write a multi-stage Dockerfile for a Node.js app. Stage 1 (<code>builder</code>): use <code>node:20-alpine</code>, install all dependencies and build the app with <code>npm run build</code>. Stage 2: use <code>node:20-alpine</code>, copy only <code>package*.json</code>, install production deps with <code>npm install --production</code>, and copy the <code>dist/</code> folder from the builder stage."
          functionSignature: "Dockerfile (multi-stage)"
          testCases:
            - input: "docker build -t myapp:optimized ."
              output: "Final image contains only production deps and built assets, no dev dependencies or source"
          hints:
            - title: "Think about it"
              content: "Multi-stage builds let you use one image for building and a different one for running. What files does your production app actually need?"
            - title: "Hint"
              content: "Name the first stage with <code>FROM node:20-alpine AS builder</code>. In the second stage, use <code>COPY --from=builder /app/dist ./dist</code> to copy only the build output."
            - title: "Pattern"
              content: |-
                <pre>1. Stage 1 (builder): full build environment
                   - Install ALL dependencies
                   - Build the application
                2. Stage 2 (runtime): lean production image
                   - Install only production dependencies
                   - Copy build artifacts from stage 1
                   - Set the CMD</pre>
          solution: |-
            FROM node:20-alpine AS builder
            WORKDIR /app
            COPY package*.json ./
            RUN npm install
            COPY . .
            RUN npm run build

            FROM node:20-alpine
            WORKDIR /app
            COPY package*.json ./
            RUN npm install --production
            COPY --from=builder /app/dist ./dist
            EXPOSE 3000
            CMD ["node", "dist/server.js"]
          difficulty: 1
        - id: v2
          title: Multi-Stage Go Build with Scratch
          description: "Write a multi-stage Dockerfile for a Go app that produces the smallest possible image. Stage 1: use <code>golang:1.22-alpine</code>, build a static binary with CGO disabled. Stage 2: use <code>scratch</code> (empty image), copy only the binary."
          functionSignature: "Dockerfile (multi-stage, scratch)"
          testCases:
            - input: "docker build -t mygoapp:scratch ."
              output: "Final image contains only the static binary, image size under 15MB"
          hints:
            - title: "Think about it"
              content: "Go can produce fully static binaries that don't need ANY runtime libraries. What's the smallest possible base image?"
            - title: "Hint"
              content: "Use <code>CGO_ENABLED=0</code> and <code>-ldflags='-s -w'</code> to build a static, stripped binary. The <code>scratch</code> image is literally empty."
            - title: "Pattern"
              content: |-
                <pre>1. Stage 1: build static binary
                   - CGO_ENABLED=0 go build -ldflags='-s -w'
                2. Stage 2: FROM scratch
                   - COPY --from=builder the binary
                   - CMD to run it</pre>
          solution: |-
            FROM golang:1.22-alpine AS builder
            WORKDIR /app
            COPY go.mod go.sum ./
            RUN go mod download
            COPY . .
            RUN CGO_ENABLED=0 go build -ldflags='-s -w' -o main .

            FROM scratch
            COPY --from=builder /app/main /main
            EXPOSE 8080
            CMD ["/main"]
          difficulty: 2
        - id: v3
          title: Optimize Python Image Size
          description: "Rewrite this Dockerfile to reduce image size. The current version uses <code>python:3.12</code> (full Debian, ~1GB). Use a slim base, add <code>--no-cache-dir</code> to pip, and clean up in the same layer:<br><pre>FROM python:3.12\nWORKDIR /app\nCOPY requirements.txt .\nRUN pip install -r requirements.txt\nCOPY . .\nCMD [\"python\", \"app.py\"]</pre>"
          functionSignature: "Dockerfile (optimized)"
          testCases:
            - input: "docker images myapp"
              output: "Image size reduced from ~1GB to ~150-200MB"
          hints:
            - title: "Think about it"
              content: "The full python:3.12 image includes compilers, dev tools, and other things you don't need at runtime. What's the smallest Python base image you can use?"
            - title: "Hint"
              content: "Use <code>python:3.12-slim</code> instead. Add <code>--no-cache-dir</code> to pip install to avoid storing downloaded packages in the image."
            - title: "Pattern"
              content: |-
                <pre>1. Use slim or alpine base image
                2. pip install --no-cache-dir
                3. Don't copy unnecessary files (use .dockerignore)</pre>
          solution: |-
            FROM python:3.12-slim
            WORKDIR /app
            COPY requirements.txt .
            RUN pip install --no-cache-dir -r requirements.txt
            COPY . .
            CMD ["python", "app.py"]
          difficulty: 1
        - id: v4
          title: Non-Root User Security
          description: "Write a Dockerfile for a Node.js app that runs as a non-root user. Use <code>node:20-alpine</code>, create a user called <code>appuser</code>, and ensure all files are owned by that user."
          functionSignature: "Dockerfile (non-root)"
          testCases:
            - input: "docker exec myapp whoami"
              output: "appuser (not root)"
          hints:
            - title: "Think about it"
              content: "Running containers as root is a security risk. If an attacker breaks out of the container, they're root on the host. How do you create and switch to a non-root user?"
            - title: "Hint"
              content: "Use <code>RUN addgroup -S appgroup && adduser -S appuser -G appgroup</code> on Alpine. Then <code>USER appuser</code> to switch. Make sure files are owned correctly."
            - title: "Pattern"
              content: |-
                <pre>1. Create group and user
                2. Set WORKDIR
                3. Copy files and install deps (as root)
                4. Change ownership: chown -R appuser:appgroup
                5. USER appuser
                6. CMD</pre>
          solution: |-
            FROM node:20-alpine
            RUN addgroup -S appgroup && adduser -S appuser -G appgroup
            WORKDIR /app
            COPY package*.json ./
            RUN npm install --production
            COPY . .
            RUN chown -R appuser:appgroup /app
            USER appuser
            EXPOSE 3000
            CMD ["node", "server.js"]
          difficulty: 2
        - id: v5
          title: Java Multi-Stage with JRE
          description: "Write a multi-stage Dockerfile for a Java app. Stage 1: use <code>eclipse-temurin:21-jdk-alpine</code> to build with Maven (<code>mvn package -DskipTests</code>). Stage 2: use <code>eclipse-temurin:21-jre-alpine</code> (JRE only, no JDK) to run the JAR."
          functionSignature: "Dockerfile (multi-stage Java)"
          testCases:
            - input: "docker images javaapp"
              output: "Final image uses JRE (~200MB) not JDK (~400MB), no Maven or source code"
          hints:
            - title: "Think about it"
              content: "You need the full JDK and Maven to build, but only the JRE to run. How can multi-stage builds help?"
            - title: "Hint"
              content: "Build stage uses JDK + Maven. Runtime stage uses JRE only. Copy just the JAR file between stages."
            - title: "Pattern"
              content: |-
                <pre>1. Stage 1 (JDK + Maven): compile and package
                   - Copy pom.xml, download deps
                   - Copy source, build JAR
                2. Stage 2 (JRE only): run
                   - Copy JAR from builder
                   - CMD java -jar</pre>
          solution: |-
            FROM eclipse-temurin:21-jdk-alpine AS builder
            WORKDIR /app
            COPY pom.xml .
            COPY src/ src/
            RUN apk add --no-cache maven && mvn package -DskipTests

            FROM eclipse-temurin:21-jre-alpine
            WORKDIR /app
            COPY --from=builder /app/target/*.jar app.jar
            EXPOSE 8080
            CMD ["java", "-jar", "app.jar"]
          difficulty: 2
        - id: v6
          title: Fix Security Issues in Dockerfile
          description: "This Dockerfile has multiple security problems. Fix them all:<br><pre>FROM node:20\nWORKDIR /app\nCOPY . .\nRUN npm install\nENV DB_PASSWORD=supersecret123\nEXPOSE 3000\nCMD node server.js</pre><br>Issues: runs as root, uses full base image, hardcoded secret, shell form CMD, no .dockerignore awareness."
          functionSignature: "Dockerfile (security hardened)"
          testCases:
            - input: "Security scan of image"
              output: "Non-root user, no hardcoded secrets, alpine base, exec form CMD"
          hints:
            - title: "Think about it"
              content: "Count the security issues: Who is running the process? What secrets are baked into the image? Is the base image larger than needed?"
            - title: "Hint"
              content: "Use alpine base, create non-root user, remove hardcoded secrets (use runtime env vars instead), use exec form CMD, order layers properly."
            - title: "Pattern"
              content: |-
                <pre>1. Use minimal base (alpine)
                2. Create non-root user
                3. Install deps before copying code
                4. Remove hardcoded secrets
                5. Switch to non-root user
                6. Use exec form CMD</pre>
          solution: |-
            FROM node:20-alpine
            RUN addgroup -S appgroup && adduser -S appuser -G appgroup
            WORKDIR /app
            COPY package*.json ./
            RUN npm install --production
            COPY . .
            RUN chown -R appuser:appgroup /app
            USER appuser
            EXPOSE 3000
            CMD ["node", "server.js"]
          difficulty: 3
        - id: v7
          title: Multi-Stage Python with Virtualenv
          description: "Write a multi-stage Dockerfile for Python that uses a virtualenv. Stage 1: install dependencies in a virtualenv. Stage 2: copy only the virtualenv and app code. This avoids needing build tools in the final image."
          functionSignature: "Dockerfile (multi-stage Python)"
          testCases:
            - input: "docker images pyapp"
              output: "Final image has no gcc/build-essential, only runtime deps and virtualenv"
          hints:
            - title: "Think about it"
              content: "Some Python packages need compilation (gcc, build headers). You need these tools to install, but not to run. How do you separate build-time from run-time?"
            - title: "Hint"
              content: "Create a virtualenv in the builder stage. Copy the entire virtualenv directory to the runtime stage. Set PATH to use the virtualenv's Python."
            - title: "Pattern"
              content: |-
                <pre>1. Stage 1 (builder): install build tools + deps
                   - python -m venv /opt/venv
                   - pip install in the venv
                2. Stage 2 (runtime): slim image
                   - COPY --from=builder /opt/venv /opt/venv
                   - ENV PATH="/opt/venv/bin:$PATH"
                   - Copy app code, run</pre>
          solution: |-
            FROM python:3.12-slim AS builder
            RUN apt-get update && apt-get install -y --no-install-recommends gcc
            WORKDIR /app
            RUN python -m venv /opt/venv
            ENV PATH="/opt/venv/bin:$PATH"
            COPY requirements.txt .
            RUN pip install --no-cache-dir -r requirements.txt

            FROM python:3.12-slim
            WORKDIR /app
            COPY --from=builder /opt/venv /opt/venv
            ENV PATH="/opt/venv/bin:$PATH"
            COPY . .
            EXPOSE 5000
            CMD ["python", "app.py"]
          difficulty: 3
        - id: v8
          title: Layer Caching Optimization
          description: "Reorder this Dockerfile to maximize layer caching. Currently, changing any source file invalidates the dependency cache:<br><pre>FROM golang:1.22-alpine\nWORKDIR /app\nCOPY . .\nRUN go mod download\nRUN go build -o main .\nEXPOSE 8080\nCMD [\"./main\"]</pre>"
          functionSignature: "Dockerfile (cache optimized)"
          testCases:
            - input: "Change main.go, rebuild"
              output: "go mod download step is cached (skipped), only build step reruns"
          hints:
            - title: "Think about it"
              content: "Docker caches each layer. If a layer changes, all subsequent layers are rebuilt. Which files change most often? Which change rarely?"
            - title: "Hint"
              content: "Copy go.mod and go.sum first, run go mod download, THEN copy the rest of the source. Dependencies rarely change, but source changes on every build."
            - title: "Pattern"
              content: |-
                <pre>1. Copy dependency manifests (go.mod, go.sum)
                2. Download dependencies (cached layer)
                3. Copy source code (changes often)
                4. Build (only reruns when source changes)</pre>
          solution: |-
            FROM golang:1.22-alpine
            WORKDIR /app
            COPY go.mod go.sum ./
            RUN go mod download
            COPY . .
            RUN go build -o main .
            EXPOSE 8080
            CMD ["./main"]
          difficulty: 1

    - id: challenge_2
      block: 1
      difficulty: 1
      concept: Pod Manifest Writing
      variants:
        - id: v1
          title: Web App Pod with Config
          description: "Write a Pod manifest for a web application named <code>webapp</code> using image <code>webapp:v2</code>. It should expose port 8080, set environment variables <code>LOG_LEVEL=info</code> and <code>PORT=8080</code>, have labels <code>app: webapp</code> and <code>version: v2</code>, and request 64Mi memory and 100m CPU."
          functionSignature: "Pod YAML"
          testCases:
            - input: "kubectl apply -f webapp.yaml"
              output: "Pod running with env vars, labels, and resource requests"
            - input: "kubectl exec webapp -- env | grep LOG_LEVEL"
              output: "LOG_LEVEL=info"
          hints:
            - title: "Think about it"
              content: "You need to combine several Pod spec features: env vars, labels, resource requests. Where does each one go in the YAML structure?"
            - title: "Hint"
              content: "Labels go in <code>metadata.labels</code>. Env vars and resources go inside <code>spec.containers[]</code>. Resource requests go under <code>resources.requests</code>."
            - title: "Pattern"
              content: |-
                <pre>apiVersion: v1
                kind: Pod
                metadata:
                  name: ...
                  labels: ...
                spec:
                  containers:
                  - name: ...
                    image: ...
                    ports: ...
                    env: ...
                    resources:
                      requests: ...</pre>
          solution: |-
            apiVersion: v1
            kind: Pod
            metadata:
              name: webapp
              labels:
                app: webapp
                version: v2
            spec:
              containers:
              - name: webapp
                image: webapp:v2
                ports:
                - containerPort: 8080
                env:
                - name: LOG_LEVEL
                  value: "info"
                - name: PORT
                  value: "8080"
                resources:
                  requests:
                    memory: "64Mi"
                    cpu: "100m"
          difficulty: 1
        - id: v2
          title: Database Pod with Persistent Volume
          description: "Write a Pod manifest for a MySQL database named <code>mysql</code> using image <code>mysql:8.0</code>. Set <code>MYSQL_ROOT_PASSWORD=rootpass</code> and <code>MYSQL_DATABASE=mydb</code>. Mount an <code>emptyDir</code> volume named <code>mysql-data</code> at <code>/var/lib/mysql</code>. Expose port 3306."
          functionSignature: "Pod YAML"
          testCases:
            - input: "kubectl apply -f mysql.yaml"
              output: "Pod running with MySQL, data volume mounted at /var/lib/mysql"
          hints:
            - title: "Think about it"
              content: "Databases need persistent storage. Even though emptyDir is not truly persistent (it's deleted with the Pod), it shows the volume mounting pattern."
            - title: "Hint"
              content: "Define volumes at <code>spec.volumes</code> level. Mount them inside containers with <code>volumeMounts</code>. An emptyDir volume has type <code>emptyDir: {}</code>."
            - title: "Pattern"
              content: |-
                <pre>spec:
                  volumes:
                  - name: data-vol
                    emptyDir: {}
                  containers:
                  - volumeMounts:
                    - name: data-vol
                      mountPath: /path</pre>
          solution: |-
            apiVersion: v1
            kind: Pod
            metadata:
              name: mysql
              labels:
                app: mysql
            spec:
              containers:
              - name: mysql
                image: mysql:8.0
                ports:
                - containerPort: 3306
                env:
                - name: MYSQL_ROOT_PASSWORD
                  value: "rootpass"
                - name: MYSQL_DATABASE
                  value: "mydb"
                volumeMounts:
                - name: mysql-data
                  mountPath: /var/lib/mysql
              volumes:
              - name: mysql-data
                emptyDir: {}
          difficulty: 2
        - id: v3
          title: Init Container Pattern
          description: "Write a Pod manifest named <code>webapp</code> that uses an init container to wait for a database before the main app starts. The init container should use <code>busybox:1.36</code> and run <code>until nslookup db-service; do echo waiting for db; sleep 2; done</code>. The main container uses <code>webapp:v1</code> on port 8080."
          functionSignature: "Pod YAML"
          testCases:
            - input: "kubectl apply -f webapp.yaml"
              output: "Init container runs first, waits for db-service DNS, then main container starts"
          hints:
            - title: "Think about it"
              content: "Init containers run before the main containers and must complete successfully. They're perfect for setup tasks or waiting for dependencies."
            - title: "Hint"
              content: "Init containers go in <code>spec.initContainers</code> (same structure as <code>containers</code>). They run in order and must exit 0 before main containers start."
            - title: "Pattern"
              content: |-
                <pre>spec:
                  initContainers:
                  - name: wait-for-db
                    image: ...
                    command: [...]
                  containers:
                  - name: main-app
                    image: ...</pre>
          solution: |-
            apiVersion: v1
            kind: Pod
            metadata:
              name: webapp
              labels:
                app: webapp
            spec:
              initContainers:
              - name: wait-for-db
                image: busybox:1.36
                command: ["/bin/sh", "-c"]
                args: ["until nslookup db-service; do echo waiting for db; sleep 2; done"]
              containers:
              - name: webapp
                image: webapp:v1
                ports:
                - containerPort: 8080
          difficulty: 2
        - id: v4
          title: Sidecar Logging Pod
          description: "Write a Pod manifest named <code>app-with-logging</code> with two containers sharing a volume. The main container <code>app</code> uses <code>busybox:1.36</code> and writes logs to <code>/var/log/app.log</code>. The sidecar <code>log-reader</code> uses <code>busybox:1.36</code> and tails the same log file. Both mount an emptyDir volume named <code>log-volume</code>."
          functionSignature: "Pod YAML"
          testCases:
            - input: "kubectl logs app-with-logging -c log-reader"
              output: "Shows the log output written by the app container"
          hints:
            - title: "Think about it"
              content: "Containers in the same Pod can share volumes. This is the classic sidecar pattern: one container generates data, another processes it."
            - title: "Hint"
              content: "Define one emptyDir volume. Mount it in both containers at the same or different paths. The app writes, the sidecar reads."
            - title: "Pattern"
              content: |-
                <pre>spec:
                  volumes:
                  - name: shared-vol
                    emptyDir: {}
                  containers:
                  - name: app (writes to vol)
                  - name: sidecar (reads from vol)</pre>
          solution: |-
            apiVersion: v1
            kind: Pod
            metadata:
              name: app-with-logging
              labels:
                app: app-with-logging
            spec:
              volumes:
              - name: log-volume
                emptyDir: {}
              containers:
              - name: app
                image: busybox:1.36
                command: ["/bin/sh", "-c"]
                args: ["while true; do echo $(date) - heartbeat >> /var/log/app.log; sleep 5; done"]
                volumeMounts:
                - name: log-volume
                  mountPath: /var/log
              - name: log-reader
                image: busybox:1.36
                command: ["/bin/sh", "-c"]
                args: ["tail -f /var/log/app.log"]
                volumeMounts:
                - name: log-volume
                  mountPath: /var/log
          difficulty: 2
        - id: v5
          title: Pod with Liveness Probe
          description: "Write a Pod manifest named <code>web</code> using image <code>nginx:1.25</code> on port 80. Add an HTTP liveness probe that checks <code>/</code> on port 80, with an initial delay of 5 seconds and a check period of 10 seconds."
          functionSignature: "Pod YAML"
          testCases:
            - input: "kubectl describe pod web"
              output: "Liveness probe configured: http-get http://:80/ delay=5s period=10s"
          hints:
            - title: "Think about it"
              content: "Liveness probes tell Kubernetes when to restart a container. If the probe fails, the container is killed and restarted."
            - title: "Hint"
              content: "Add <code>livenessProbe</code> to the container spec. Use <code>httpGet</code> with <code>path</code> and <code>port</code>. Set <code>initialDelaySeconds</code> and <code>periodSeconds</code>."
            - title: "Pattern"
              content: |-
                <pre>containers:
                - name: ...
                  livenessProbe:
                    httpGet:
                      path: /
                      port: 80
                    initialDelaySeconds: 5
                    periodSeconds: 10</pre>
          solution: |-
            apiVersion: v1
            kind: Pod
            metadata:
              name: web
              labels:
                app: web
            spec:
              containers:
              - name: web
                image: nginx:1.25
                ports:
                - containerPort: 80
                livenessProbe:
                  httpGet:
                    path: /
                    port: 80
                  initialDelaySeconds: 5
                  periodSeconds: 10
          difficulty: 2
        - id: v6
          title: Pod with Readiness and Liveness Probes
          description: "Write a Pod manifest named <code>api</code> using image <code>api:v1</code> on port 8080. Add both a readiness probe (HTTP GET <code>/healthz</code> on port 8080, initial delay 3s, period 5s) and a liveness probe (HTTP GET <code>/healthz</code> on port 8080, initial delay 10s, period 15s)."
          functionSignature: "Pod YAML"
          testCases:
            - input: "kubectl describe pod api"
              output: "Both readiness and liveness probes configured on /healthz"
          hints:
            - title: "Think about it"
              content: "Readiness probes control traffic routing (is the pod ready to receive traffic?). Liveness probes control restarts (is the pod still alive?). You usually want both."
            - title: "Hint"
              content: "Add both <code>readinessProbe</code> and <code>livenessProbe</code> to the container. Readiness should have a shorter initial delay so traffic routing is quick."
            - title: "Pattern"
              content: |-
                <pre>containers:
                - name: api
                  readinessProbe:
                    httpGet: ...
                    initialDelaySeconds: 3
                  livenessProbe:
                    httpGet: ...
                    initialDelaySeconds: 10</pre>
          solution: |-
            apiVersion: v1
            kind: Pod
            metadata:
              name: api
              labels:
                app: api
            spec:
              containers:
              - name: api
                image: api:v1
                ports:
                - containerPort: 8080
                readinessProbe:
                  httpGet:
                    path: /healthz
                    port: 8080
                  initialDelaySeconds: 3
                  periodSeconds: 5
                livenessProbe:
                  httpGet:
                    path: /healthz
                    port: 8080
                  initialDelaySeconds: 10
                  periodSeconds: 15
          difficulty: 3
        - id: v7
          title: Complete Application Pod
          description: "Write a Pod manifest named <code>fullapp</code> using image <code>fullapp:v1</code>. Include: labels (<code>app: fullapp</code>, <code>env: staging</code>), port 8080, env vars (<code>DB_HOST=db-service</code>, <code>REDIS_HOST=redis-service</code>, <code>LOG_LEVEL=debug</code>), resource requests (128Mi/250m) and limits (256Mi/500m), a readiness probe on <code>/ready</code>, and a liveness probe on <code>/health</code>."
          functionSignature: "Pod YAML"
          testCases:
            - input: "kubectl apply -f fullapp.yaml && kubectl describe pod fullapp"
              output: "Pod with all env vars, resource constraints, both probes, correct labels"
          hints:
            - title: "Think about it"
              content: "This combines everything: metadata, env vars, resources, and probes. Take it section by section."
            - title: "Hint"
              content: "Build the manifest piece by piece: start with the basic structure (apiVersion, kind, metadata), then add the container with ports, env, resources, and finally probes."
            - title: "Pattern"
              content: |-
                <pre>1. metadata: name, labels
                2. container: image, ports
                3. env: list of name/value pairs
                4. resources: requests and limits
                5. readinessProbe and livenessProbe</pre>
          solution: |-
            apiVersion: v1
            kind: Pod
            metadata:
              name: fullapp
              labels:
                app: fullapp
                env: staging
            spec:
              containers:
              - name: fullapp
                image: fullapp:v1
                ports:
                - containerPort: 8080
                env:
                - name: DB_HOST
                  value: "db-service"
                - name: REDIS_HOST
                  value: "redis-service"
                - name: LOG_LEVEL
                  value: "debug"
                resources:
                  requests:
                    memory: "128Mi"
                    cpu: "250m"
                  limits:
                    memory: "256Mi"
                    cpu: "500m"
                readinessProbe:
                  httpGet:
                    path: /ready
                    port: 8080
                  initialDelaySeconds: 5
                  periodSeconds: 5
                livenessProbe:
                  httpGet:
                    path: /health
                    port: 8080
                  initialDelaySeconds: 10
                  periodSeconds: 15
          difficulty: 3

    - id: challenge_3
      block: 2
      difficulty: 2
      concept: kubectl Troubleshooting
      variants:
        - id: v1
          title: Diagnose ImagePullBackOff
          description: "A Pod is stuck in <code>ImagePullBackOff</code>. The <code>kubectl describe pod myapp</code> output shows:<br><pre>Events:\n  Warning  Failed  pull image \"myapp:latset\": rpc error: failed to pull and unpack image: not found\n  Warning  Failed  Error: ImagePullBackOff</pre><br>What is wrong and what kubectl command would fix it?"
          functionSignature: "kubectl command"
          testCases:
            - input: "kubectl describe pod myapp shows ImagePullBackOff"
              output: "Image tag has a typo: 'latset' should be 'latest'. Fix the image name."
          hints:
            - title: "Think about it"
              content: "ImagePullBackOff means Kubernetes cannot pull the container image. Look carefully at the image name in the error message."
            - title: "Hint"
              content: "The image tag has a typo: <code>latset</code> instead of <code>latest</code>. Delete the pod and recreate with the correct image, or edit the pod spec."
            - title: "Pattern"
              content: |-
                <pre>1. Read the error message carefully
                2. Check image name and tag for typos
                3. Fix: delete pod, correct YAML, re-apply
                kubectl delete pod myapp
                # Fix image in YAML to myapp:latest
                kubectl apply -f pod.yaml</pre>
          solution: |-
            # The image tag has a typo: "latset" should be "latest"
            # Fix the Pod YAML and re-apply:
            kubectl delete pod myapp
            # In pod.yaml, change image: myapp:latset to image: myapp:latest
            kubectl apply -f pod.yaml
          difficulty: 2
        - id: v2
          title: Diagnose CrashLoopBackOff
          description: "A Pod is in <code>CrashLoopBackOff</code> with 5 restarts. The <code>kubectl logs myapp</code> output shows:<br><pre>Error: Cannot find module '/app/server.js'\n    at Module._resolveFilename (node:internal/modules/cjs/loader:1075:15)</pre><br>What is wrong and how do you investigate further?"
          functionSignature: "kubectl commands"
          testCases:
            - input: "kubectl get pods shows CrashLoopBackOff, RESTARTS=5"
              output: "The application file server.js is missing from the container image. Check Dockerfile COPY instructions."
          hints:
            - title: "Think about it"
              content: "CrashLoopBackOff means the container starts, crashes, and Kubernetes keeps restarting it. The logs show why it's crashing."
            - title: "Hint"
              content: "The Node.js app can't find server.js. This is likely a Dockerfile issue: the COPY instruction might be missing files, or the WORKDIR is wrong."
            - title: "Pattern"
              content: |-
                <pre>1. kubectl logs myapp -- read crash logs
                2. kubectl logs myapp --previous -- read previous crash
                3. Identify: missing file in container
                4. Fix: check Dockerfile COPY and WORKDIR
                5. Rebuild image, update pod</pre>
          solution: |-
            # Check current and previous logs
            kubectl logs myapp
            kubectl logs myapp --previous
            # The error shows /app/server.js is missing
            # Fix: check Dockerfile COPY instruction and rebuild
            # Ensure COPY . . copies server.js to /app/
            docker build -t myapp:v1 .
            kubectl delete pod myapp
            kubectl apply -f pod.yaml
          difficulty: 2
        - id: v3
          title: Diagnose Pending Pod
          description: "A Pod is stuck in <code>Pending</code> state. The <code>kubectl describe pod myapp</code> shows:<br><pre>Events:\n  Warning  FailedScheduling  0/1 nodes are available: 1 Insufficient memory. preemption: 0/1 nodes are available: 1 No preemption victims found for incoming pod.</pre><br>What is wrong and what are your options?"
          functionSignature: "kubectl commands"
          testCases:
            - input: "kubectl get pods shows STATUS=Pending"
              output: "Pod requests more memory than any node has available. Reduce resource requests or add capacity."
          hints:
            - title: "Think about it"
              content: "Pending means the scheduler cannot find a node to place the Pod. The event message tells you why."
            - title: "Hint"
              content: "The node doesn't have enough memory for this Pod's resource requests. Options: reduce the memory request in the Pod spec, free up memory by deleting other pods, or add more nodes."
            - title: "Pattern"
              content: |-
                <pre>1. kubectl describe pod -- read FailedScheduling reason
                2. kubectl describe node -- check allocatable resources
                3. kubectl top nodes -- check actual usage
                4. Fix options:
                   - Reduce resource requests
                   - Delete unused pods
                   - Add node capacity</pre>
          solution: |-
            # Check what resources the node has
            kubectl describe node | grep -A 5 "Allocated resources"
            kubectl top nodes
            # Option 1: Reduce Pod memory request in YAML
            # Option 2: Delete other pods to free resources
            kubectl get pods --all-namespaces
            # Option 3: Add more nodes to the cluster
          difficulty: 2
        - id: v4
          title: Diagnose CreateContainerConfigError
          description: "A Pod is in <code>CreateContainerConfigError</code>. The <code>kubectl describe pod myapp</code> shows:<br><pre>Events:\n  Warning  Failed  Error: secret \"db-credentials\" not found</pre><br>The Pod spec references a secret for environment variables. What is wrong?"
          functionSignature: "kubectl commands"
          testCases:
            - input: "kubectl describe pod myapp shows CreateContainerConfigError"
              output: "Pod references secret 'db-credentials' which doesn't exist. Create the secret first."
          hints:
            - title: "Think about it"
              content: "CreateContainerConfigError means Kubernetes can't configure the container before starting it. Usually a missing ConfigMap or Secret."
            - title: "Hint"
              content: "The Pod references a Secret called <code>db-credentials</code> that doesn't exist. Create it with <code>kubectl create secret</code>."
            - title: "Pattern"
              content: |-
                <pre>1. kubectl describe pod -- find the missing resource
                2. kubectl get secrets -- verify it's missing
                3. Create the missing secret:
                   kubectl create secret generic db-credentials \
                     --from-literal=password=mypass</pre>
          solution: |-
            # Verify the secret doesn't exist
            kubectl get secret db-credentials
            # Create the missing secret
            kubectl create secret generic db-credentials \
              --from-literal=username=admin \
              --from-literal=password=mypass
            # The pod should start automatically once the secret exists
            kubectl get pods -w
          difficulty: 2
        - id: v5
          title: Diagnose Pod OOMKilled
          description: "A Pod keeps restarting. <code>kubectl get pods</code> shows <code>RESTARTS=8</code> and <code>kubectl describe pod myapp</code> shows:<br><pre>Last State:     Terminated\n  Reason:       OOMKilled\n  Exit Code:    137</pre><br>The Pod has <code>resources.limits.memory: 64Mi</code>. What happened?"
          functionSignature: "kubectl commands"
          testCases:
            - input: "kubectl describe pod shows OOMKilled, Exit Code 137"
              output: "Container exceeded its 64Mi memory limit. Increase memory limit or fix memory leak in application."
          hints:
            - title: "Think about it"
              content: "OOMKilled means Out Of Memory Killed. Exit code 137 = 128 + 9 (SIGKILL). The container used more memory than its limit allows."
            - title: "Hint"
              content: "The memory limit of 64Mi is too low for the application. Either increase the limit or investigate why the app uses so much memory. Use <code>kubectl top pod</code> to see current usage."
            - title: "Pattern"
              content: |-
                <pre>1. kubectl describe pod -- confirm OOMKilled
                2. kubectl top pod -- check memory usage
                3. Fix options:
                   - Increase memory limit in Pod spec
                   - Fix memory leak in application
                   - Profile app memory usage</pre>
          solution: |-
            # Check current memory usage
            kubectl top pod myapp
            # Increase the memory limit in the Pod YAML:
            # resources:
            #   limits:
            #     memory: "256Mi"
            #   requests:
            #     memory: "128Mi"
            kubectl delete pod myapp
            kubectl apply -f pod.yaml
          difficulty: 3
        - id: v6
          title: Diagnose Wrong Port Configuration
          description: "A Pod is running but <code>kubectl port-forward pod/myapp 8080:3000</code> results in a connection refused error. The Pod YAML has <code>containerPort: 3000</code> but the app actually listens on port 8080. What is wrong?"
          functionSignature: "kubectl commands"
          testCases:
            - input: "kubectl port-forward pod/myapp 8080:3000 -> connection refused"
              output: "containerPort in YAML (3000) doesn't match actual app port (8080). Fix port-forward or YAML."
          hints:
            - title: "Think about it"
              content: "<code>containerPort</code> in the Pod spec is informational only -- it doesn't actually open or restrict ports. The real issue is which port the app is listening on."
            - title: "Hint"
              content: "The port-forward is targeting port 3000 in the container, but the app listens on 8080. Either change the port-forward command or update the Pod YAML to match the actual port."
            - title: "Pattern"
              content: |-
                <pre>1. kubectl logs myapp -- check what port app reports
                2. Quick fix: kubectl port-forward pod/myapp 8080:8080
                3. Proper fix: update Pod YAML containerPort to 8080
                4. Rebuild/redeploy if app port needs changing</pre>
          solution: |-
            # Check what port the app is actually listening on
            kubectl logs myapp
            # Quick fix: port-forward to the correct port
            kubectl port-forward pod/myapp 8080:8080
            # Proper fix: update Pod YAML to use correct containerPort
            # Then re-apply
            kubectl apply -f pod.yaml
          difficulty: 2
        - id: v7
          title: Diagnose ErrImageNeverPull
          description: "A Pod shows <code>ErrImageNeverPull</code>. The describe output shows:<br><pre>Events:\n  Warning  ErrImageNeverPull  Container image \"myapp:dev\" is not present with pull policy of Never</pre><br>The Pod spec has <code>imagePullPolicy: Never</code>. What went wrong and how do you fix it for a local development workflow?"
          functionSignature: "kubectl commands"
          testCases:
            - input: "kubectl describe pod shows ErrImageNeverPull"
              output: "Image not available locally on the node. Build/load the image into the cluster's Docker daemon."
          hints:
            - title: "Think about it"
              content: "<code>imagePullPolicy: Never</code> means don't try to pull from a registry -- only use images already on the node. If the image isn't there, it fails."
            - title: "Hint"
              content: "For minikube: use <code>eval $(minikube docker-env)</code> to point your Docker CLI to minikube's daemon, then build the image. For kind: use <code>kind load docker-image</code>."
            - title: "Pattern"
              content: |-
                <pre># For minikube:
                eval $(minikube docker-env)
                docker build -t myapp:dev .

                # For kind:
                docker build -t myapp:dev .
                kind load docker-image myapp:dev

                # Then re-create the pod
                kubectl delete pod myapp
                kubectl apply -f pod.yaml</pre>
          solution: |-
            # For minikube - build inside minikube's Docker:
            eval $(minikube docker-env)
            docker build -t myapp:dev .

            # For kind - load the image into the cluster:
            docker build -t myapp:dev .
            kind load docker-image myapp:dev --name learn-k8s

            # Re-create the pod
            kubectl delete pod myapp
            kubectl apply -f pod.yaml
          difficulty: 3
