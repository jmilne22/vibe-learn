conceptLinks:
  Parallel Sum: "#lesson-goroutines"
  Timeout Pattern: "#lesson-select"
  Fan-Out Fan-In: "#lesson-fan-out-fan-in"
  Rate Limiter: "#lesson-ticker"
  Graceful Shutdown: "#lesson-signals"
sharedContent: {}
variants:
  warmups:
    - id: warmup_1
      concept: Parallel Sum
      variants:
        - id: v1
          title: Parallel Sum
          description: >-
            Write <code>func parallelSum(nums []int, workers int) int</code> that splits a slice into
            <code>workers</code> chunks, sums each chunk in a separate goroutine, and combines the partial sums via a
            channel. Return the total sum.
          hints:
            - Create a buffered channel <code>make(chan int, workers)</code> for partial sums.
            - Calculate <code>chunkSize := len(nums) / workers</code> and handle the last chunk getting any remainder.
            - Launch <code>workers</code> goroutines, then receive <code>workers</code> results.
          solution: |-
            func parallelSum(nums []int, workers int) int {
                ch := make(chan int, workers)
                chunkSize := len(nums) / workers

                for i := 0; i < workers; i++ {
                    start := i * chunkSize
                    end := start + chunkSize
                    if i == workers-1 {
                        end = len(nums)
                    }
                    go func(chunk []int) {
                        sum := 0
                        for _, n := range chunk {
                            sum += n
                        }
                        ch <- sum
                    }(nums[start:end])
                }

                total := 0
                for i := 0; i < workers; i++ {
                    total += <-ch
                }
                return total
            }
          annotations:
            - type: idiom
              label: Buffered Channel
              text: >-
                Using a buffered channel sized to the number of workers prevents goroutines from blocking on send. Each
                goroutine sends once and exits.
            - type: gotcha
              label: Slice Sharing
              text: >-
                Pass the sub-slice as a function argument to the goroutine closure. Capturing loop variables directly
                causes a data race.
        - id: v2
          title: Parallel Max
          description: >-
            Write <code>func parallelMax(nums []int, workers int) int</code> that splits a slice into
            <code>workers</code> chunks, finds the maximum in each chunk using a goroutine, and returns the overall
            maximum.
          hints:
            - Each goroutine finds the max of its chunk and sends it on a channel.
            - The main goroutine receives all partial max values and finds the global max.
            - Initialize your result with the first received value, then compare.
          solution: |-
            func parallelMax(nums []int, workers int) int {
                ch := make(chan int, workers)
                chunkSize := len(nums) / workers

                for i := 0; i < workers; i++ {
                    start := i * chunkSize
                    end := start + chunkSize
                    if i == workers-1 {
                        end = len(nums)
                    }
                    go func(chunk []int) {
                        mx := chunk[0]
                        for _, n := range chunk[1:] {
                            if n > mx {
                                mx = n
                            }
                        }
                        ch <- mx
                    }(nums[start:end])
                }

                result := <-ch
                for i := 1; i < workers; i++ {
                    if v := <-ch; v > result {
                        result = v
                    }
                }
                return result
            }
          annotations:
            - type: complexity
              label: O(n)
              text: >-
                Each element is visited exactly once across all goroutines, plus O(workers) to combine results. Total is
                O(n).
            - type: gotcha
              label: Empty Chunk
              text: >-
                If len(nums) < workers, some chunks will be empty. Guard against indexing chunk[0] on an empty slice to
                avoid a panic.
        - id: v3
          title: Parallel Word Count
          description: >-
            Write <code>func parallelWordCount(texts []string, workers int) int</code> that splits a slice of text
            strings into <code>workers</code> chunks, counts words in each chunk in a goroutine (using
            <code>strings.Fields</code>), and returns the total word count.
          hints:
            - Use <code>strings.Fields(text)</code> to split text into words.
            - Each goroutine sums <code>len(strings.Fields(text))</code> for its chunk of texts.
            - Combine partial counts the same way as parallel sum.
          solution: |-
            func parallelWordCount(texts []string, workers int) int {
                ch := make(chan int, workers)
                chunkSize := len(texts) / workers

                for i := 0; i < workers; i++ {
                    start := i * chunkSize
                    end := start + chunkSize
                    if i == workers-1 {
                        end = len(texts)
                    }
                    go func(chunk []string) {
                        count := 0
                        for _, text := range chunk {
                            count += len(strings.Fields(text))
                        }
                        ch <- count
                    }(texts[start:end])
                }

                total := 0
                for i := 0; i < workers; i++ {
                    total += <-ch
                }
                return total
            }
          annotations:
            - type: stdlib
              label: strings.Fields
              text: >-
                strings.Fields splits a string around whitespace and is more robust than strings.Split(s, " ") because
                it handles multiple spaces and leading/trailing whitespace.
            - type: idiom
              label: Map-Reduce
              text: >-
                This is a basic map-reduce: map each text to a word count in parallel, then reduce by summing. Go's
                goroutines and channels make this pattern natural.
        - id: v4
          title: Parallel Filter
          description: >-
            Write <code>func parallelFilter(nums []int, workers int) []int</code> that splits a slice into
            <code>workers</code> chunks, filters for even numbers in each chunk using a goroutine, and merges the
            results into a single slice.
          hints:
            - Each goroutine sends a <code>[]int</code> of even numbers (not individual ints).
            - Use a buffered channel of type <code>chan []int</code>.
            - Append each partial result to the final slice.
          solution: |-
            func parallelFilter(nums []int, workers int) []int {
                ch := make(chan []int, workers)
                chunkSize := len(nums) / workers

                for i := 0; i < workers; i++ {
                    start := i * chunkSize
                    end := start + chunkSize
                    if i == workers-1 {
                        end = len(nums)
                    }
                    go func(chunk []int) {
                        var evens []int
                        for _, n := range chunk {
                            if n%2 == 0 {
                                evens = append(evens, n)
                            }
                        }
                        ch <- evens
                    }(nums[start:end])
                }

                var result []int
                for i := 0; i < workers; i++ {
                    result = append(result, <-ch...)
                }
                return result
            }
          annotations:
            - type: gotcha
              label: Order Not Guaranteed
              text: >-
                Goroutines may complete in any order. The result slice may not match the original order. If order
                matters, use indexed results or sort afterward.
            - type: idiom
              label: Slice of Slices
              text: >-
                Sending []int per goroutine is more efficient than sending individual ints. It reduces channel
                operations and contention.
    - id: warmup_2
      concept: Timeout Pattern
      variants:
        - id: v1
          title: Timeout Pattern
          description: >-
            Write <code>func doWithTimeout(timeout time.Duration) (string, error)</code> that launches a slow operation
            in a goroutine. Use <code>select</code> with <code>time.After</code> to return the result if it completes in
            time, or an error if it times out.
          hints:
            - Create a buffered channel <code>ch := make(chan string, 1)</code> so the goroutine does not leak.
            - >-
              Use <code>select</code> with two cases: <code>case result := &lt;-ch</code> and <code>case
              &lt;-time.After(timeout)</code>.
            - Return <code>fmt.Errorf("operation timed out")</code> in the timeout case.
          solution: |-
            func doWithTimeout(timeout time.Duration) (string, error) {
                ch := make(chan string, 1)
                go func() {
                    result := slowOperation()
                    ch <- result
                }()

                select {
                case result := <-ch:
                    return result, nil
                case <-time.After(timeout):
                    return "", fmt.Errorf("operation timed out")
                }
            }
          annotations:
            - type: idiom
              label: Select + time.After
              text: >-
                The select statement with time.After is the standard Go timeout pattern. It cleanly handles the race
                between completion and deadline.
            - type: gotcha
              label: Goroutine Leak
              text: >-
                Use a buffered channel (size 1) so the goroutine can send even after timeout. Otherwise it blocks
                forever, leaking the goroutine.
        - id: v2
          title: Context Timeout
          description: >-
            Write <code>func fetchWithContext(ctx context.Context) (string, error)</code> that launches a slow operation
            in a goroutine. Use <code>select</code> to return the result or return an error if the context is cancelled
            or times out.
          hints:
            - Use <code>ctx.Done()</code> in the select to detect cancellation.
            - Use <code>ctx.Err()</code> to get the reason (deadline exceeded or cancelled).
            - >-
              The caller creates the context: <code>ctx, cancel := context.WithTimeout(context.Background(),
              2*time.Second)</code>.
          solution: |-
            func fetchWithContext(ctx context.Context) (string, error) {
                ch := make(chan string, 1)
                go func() {
                    result := slowOperation()
                    ch <- result
                }()

                select {
                case result := <-ch:
                    return result, nil
                case <-ctx.Done():
                    return "", ctx.Err()
                }
            }
          annotations:
            - type: idiom
              label: Context for Cancellation
              text: >-
                Using context.Context is the preferred Go pattern for timeouts and cancellation. It propagates through
                call chains and supports both deadlines and manual cancellation.
            - type: stdlib
              label: context Package
              text: >-
                context.WithTimeout and context.WithCancel create derived contexts. Always call the cancel function
                (usually via defer) to release resources.
        - id: v3
          title: First Response Wins
          description: >-
            Write <code>func fetchFirst(urls []string, timeout time.Duration) (string, error)</code> that launches a
            goroutine for each URL (simulated by sleeping a random duration and returning the URL). Return whichever
            result arrives first, or an error if all timeout.
          hints:
            - Create a single channel and have all goroutines send to it.
            - The first <code>select</code> case that fires is the fastest response.
            - Use <code>time.After(timeout)</code> as the fallback.
          solution: |-
            func fetchFirst(urls []string, timeout time.Duration) (string, error) {
                ch := make(chan string, len(urls))
                for _, url := range urls {
                    go func(u string) {
                        // simulate variable latency
                        time.Sleep(time.Duration(rand.Intn(100)) * time.Millisecond)
                        ch <- u
                    }(url)
                }

                select {
                case result := <-ch:
                    return result, nil
                case <-time.After(timeout):
                    return "", fmt.Errorf("all requests timed out")
                }
            }
          annotations:
            - type: pattern
              label: First Response Wins
              text: >-
                Sending redundant requests to multiple backends and taking the first response is a latency optimization
                pattern. Go's select makes this trivial.
            - type: gotcha
              label: Goroutine Cleanup
              text: >-
                The losing goroutines still complete and send to the buffered channel. The buffer prevents them from
                leaking, but they still consume resources until they finish.
        - id: v4
          title: Retry with Timeout
          description: >-
            Write <code>func retryWithTimeout(maxRetries int, timeout time.Duration, fn func() (string, error)) (string,
            error)</code> that retries <code>fn</code> up to <code>maxRetries</code> times. Each attempt has the given
            timeout. Return the first successful result or the last error.
          hints:
            - Loop up to <code>maxRetries</code>.
            - For each attempt, run <code>fn</code> in a goroutine with a <code>select</code> timeout.
            - If it succeeds, return immediately. If all retries fail, return the last error.
          solution: |-
            func retryWithTimeout(maxRetries int, timeout time.Duration, fn func() (string, error)) (string, error) {
                var lastErr error
                for i := 0; i < maxRetries; i++ {
                    type result struct {
                        val string
                        err error
                    }
                    ch := make(chan result, 1)
                    go func() {
                        v, err := fn()
                        ch <- result{v, err}
                    }()

                    select {
                    case r := <-ch:
                        if r.err == nil {
                            return r.val, nil
                        }
                        lastErr = r.err
                    case <-time.After(timeout):
                        lastErr = fmt.Errorf("attempt %d timed out", i+1)
                    }
                }
                return "", fmt.Errorf("all %d retries failed: %w", maxRetries, lastErr)
            }
          annotations:
            - type: pattern
              label: Retry Pattern
              text: >-
                Combining retry loops with per-attempt timeouts is a resilience pattern. Each attempt gets a fresh
                timeout rather than sharing a global deadline.
            - type: idiom
              label: Anonymous Struct
              text: >-
                Using a local anonymous struct <code>type result struct{...}</code> inside the function is idiomatic Go
                for grouping return values through a channel.
  challenges:
    - id: challenge_1
      block: 1
      difficulty: 3
      concept: Fan-Out Fan-In
      docLinks:
        - url: https://pkg.go.dev/sync#WaitGroup
          title: Package sync.WaitGroup
          note: coordinating goroutines
        - url: https://go.dev/blog/pipelines
          title: "Go Blog: Pipelines and cancellation"
          note: fan-out fan-in pattern
      variants:
        - id: v1
          title: Merge Channels
          description: >-
            Write <code>func merge(channels ...&lt;-chan int) &lt;-chan int</code> that takes multiple input channels
            and merges all their values into a single output channel. The output channel should close when all inputs
            are exhausted. Use a <code>sync.WaitGroup</code> to track completion.
          functionSignature: func merge(channels ...<-chan int) <-chan int
          testCases:
            - input: ch1 sends 1,2; ch2 sends 3,4; ch3 sends 5,6
              output: merged channel produces all 6 values (order may vary)
            - input: ch1 sends 10; ch2 sends 20
              output: merged channel produces 10 and 20
          hints:
            - title: ðŸ¤” Think about it
              content: >-
                How do you know when all input channels are done? How do you safely close the output channel exactly
                once?
            - title: ðŸ’¡ Hint
              content: >-
                Use sync.WaitGroup: Add(1) for each input channel, Done() when a reader goroutine finishes. A separate
                goroutine waits on wg.Wait() then closes the output.
            - title: ðŸ”§ Pattern
              content: >-
                <pre>1. Create output channel

                2. For each input: wg.Add(1), launch goroutine that ranges over input and sends to output, defer
                wg.Done()

                3. Launch goroutine: wg.Wait(), close(output)

                4. Return output</pre>
          solution: |-
            func merge(channels ...<-chan int) <-chan int {
                out := make(chan int)
                var wg sync.WaitGroup

                for _, ch := range channels {
                    wg.Add(1)
                    go func(c <-chan int) {
                        defer wg.Done()
                        for n := range c {
                            out <- n
                        }
                    }(ch)
                }

                go func() {
                    wg.Wait()
                    close(out)
                }()

                return out
            }
          difficulty: 3
          annotations:
            - type: pattern
              label: Fan-In
              text: >-
                Fan-in merges multiple channels into one. This is a core Go concurrency pattern used in pipelines, event
                systems, and multiplexing.
            - type: idiom
              label: WaitGroup + Close
              text: >-
                The WaitGroup/close pattern ensures the output channel closes exactly once after all inputs are drained.
                Never close a channel from the receiver side.
            - type: gotcha
              label: Channel Direction
              text: >-
                Use <-chan int (receive-only) in the function signature to prevent the merge function from accidentally
                sending to input channels.
        - id: v2
          title: Pipeline Stages
          description: >-
            Build a three-stage pipeline: <code>func generate(nums ...int) &lt;-chan int</code> sends numbers,
            <code>func square(in &lt;-chan int) &lt;-chan int</code> squares each, <code>func filter(in &lt;-chan int,
            min int) &lt;-chan int</code> keeps only values >= min. Each stage runs in its own goroutine and closes its
            output when done.
          functionSignature: func generate(nums ...int) <-chan int
          testCases:
            - input: generate(1,2,3,4,5) -> square -> filter(_, 10)
              output: "channel produces: 16, 25"
            - input: generate(2,3) -> square -> filter(_, 5)
              output: "channel produces: 9"
          hints:
            - title: ðŸ¤” Think about it
              content: How does each stage know when to stop? What happens when you range over a closed channel?
            - title: ðŸ’¡ Hint
              content: >-
                Each stage reads from its input channel using range (which stops when the channel closes), processes the
                value, and sends to its output channel. Close the output when the range loop ends.
            - title: ðŸ”§ Pattern
              content: |-
                <pre>1. generate: send each num, close output
                2. square: range over input, send n*n, close output
                3. filter: range over input, send if >= min, close output
                4. Wire: filter(square(generate(nums...)), min)</pre>
          solution: |-
            func generate(nums ...int) <-chan int {
                out := make(chan int)
                go func() {
                    for _, n := range nums {
                        out <- n
                    }
                    close(out)
                }()
                return out
            }

            func square(in <-chan int) <-chan int {
                out := make(chan int)
                go func() {
                    for n := range in {
                        out <- n * n
                    }
                    close(out)
                }()
                return out
            }

            func filter(in <-chan int, min int) <-chan int {
                out := make(chan int)
                go func() {
                    for n := range in {
                        if n >= min {
                            out <- n
                        }
                    }
                    close(out)
                }()
                return out
            }
          difficulty: 3
          annotations:
            - type: pattern
              label: Pipeline Pattern
              text: >-
                Go pipelines connect stages via channels. Each stage is a goroutine that reads, processes, and writes.
                Closing channels propagates completion through the pipeline.
            - type: idiom
              label: Range Over Channel
              text: >-
                Using for n := range ch reads until the channel is closed. This is the idiomatic way to consume a
                channel and avoids infinite loops.
        - id: v3
          title: Worker Pool
          description: >-
            Write <code>func workerPool(jobs []int, numWorkers int) []int</code> that processes jobs through a pool of
            workers. Each worker reads from a shared jobs channel, doubles the value, and sends the result to a results
            channel. Return all results.
          functionSignature: func workerPool(jobs []int, numWorkers int) []int
          testCases:
            - input: "[]int{1, 2, 3, 4, 5}, 3"
              output: "[]int{2, 4, 6, 8, 10} (order may vary)"
            - input: "[]int{10, 20}, 2"
              output: "[]int{20, 40} (order may vary)"
          hints:
            - title: ðŸ¤” Think about it
              content: How do multiple workers share work? Who closes the jobs channel, and who closes the results channel?
            - title: ðŸ’¡ Hint
              content: >-
                Send all jobs to a jobs channel, then close it. Workers range over the jobs channel. Use a WaitGroup to
                know when all workers are done, then close the results channel.
            - title: ðŸ”§ Pattern
              content: |-
                <pre>1. Create jobs and results channels
                2. Launch numWorkers goroutines that range over jobs
                3. Send all jobs, close jobs channel
                4. WaitGroup: wg.Wait() then close results
                5. Collect results with range</pre>
          solution: |-
            func workerPool(jobs []int, numWorkers int) []int {
                jobsCh := make(chan int, len(jobs))
                resultsCh := make(chan int, len(jobs))

                var wg sync.WaitGroup
                for i := 0; i < numWorkers; i++ {
                    wg.Add(1)
                    go func() {
                        defer wg.Done()
                        for job := range jobsCh {
                            resultsCh <- job * 2
                        }
                    }()
                }

                for _, job := range jobs {
                    jobsCh <- job
                }
                close(jobsCh)

                go func() {
                    wg.Wait()
                    close(resultsCh)
                }()

                var results []int
                for r := range resultsCh {
                    results = append(results, r)
                }
                return results
            }
          difficulty: 3
          annotations:
            - type: pattern
              label: Worker Pool
              text: >-
                The worker pool pattern bounds concurrency. N workers share a jobs channel, limiting how many tasks run
                simultaneously. This prevents resource exhaustion.
            - type: gotcha
              label: Channel Close Order
              text: >-
                Close the jobs channel after sending all jobs (so workers stop). Close the results channel after all
                workers finish (so the collector stops). Wrong order causes deadlock or panic.
        - id: v4
          title: Fan-Out Map
          description: >-
            Write <code>func fanOutMap(inputs []string, workers int, fn func(string) string) []string</code> that
            distributes strings to <code>workers</code> goroutines, applies <code>fn</code> to each, and collects
            results preserving original order.
          functionSignature: func fanOutMap(inputs []string, workers int, fn func(string) string) []string
          testCases:
            - input: "[]string{\"hello\", \"world\"}, 2, strings.ToUpper"
              output: "[\"HELLO\", \"WORLD\"]"
            - input: "[]string{\"a\", \"b\", \"c\"}, 2, func(s string) string { return s + \"!\" }"
              output: "[\"a!\", \"b!\", \"c!\"]"
          hints:
            - title: ðŸ¤” Think about it
              content: >-
                How do you preserve order when goroutines complete at different times? What extra information do you
                need to send with each result?
            - title: ðŸ’¡ Hint
              content: >-
                Send indexed jobs (index + input) to workers. Workers return indexed results (index + output).
                Pre-allocate the result slice and place each result at its original index.
            - title: ðŸ”§ Pattern
              content: |-
                <pre>1. Create indexed job struct {index int, value string}
                2. Workers read jobs, apply fn, send indexed results
                3. Pre-allocate results := make([]string, len(inputs))
                4. Place each result at results[index]</pre>
          solution: |-
            func fanOutMap(inputs []string, workers int, fn func(string) string) []string {
                type job struct {
                    index int
                    value string
                }
                type result struct {
                    index int
                    value string
                }

                jobsCh := make(chan job, len(inputs))
                resultsCh := make(chan result, len(inputs))

                var wg sync.WaitGroup
                for i := 0; i < workers; i++ {
                    wg.Add(1)
                    go func() {
                        defer wg.Done()
                        for j := range jobsCh {
                            resultsCh <- result{index: j.index, value: fn(j.value)}
                        }
                    }()
                }

                for i, input := range inputs {
                    jobsCh <- job{index: i, value: input}
                }
                close(jobsCh)

                go func() {
                    wg.Wait()
                    close(resultsCh)
                }()

                results := make([]string, len(inputs))
                for r := range resultsCh {
                    results[r.index] = r.value
                }
                return results
            }
          difficulty: 3
          annotations:
            - type: pattern
              label: Ordered Fan-Out
              text: >-
                By pairing each job with its index, you can reconstruct the original order despite concurrent
                processing. This is essential when output order must match input order.
            - type: complexity
              label: O(n)
              text: >-
                Each input is processed exactly once. The worker pool bounds parallelism to 'workers' goroutines while
                still processing all n items.
    - id: challenge_2
      block: 2
      difficulty: 3
      concept: Rate Limiter
      docLinks:
        - url: https://pkg.go.dev/time#Ticker
          title: Package time.Ticker
          note: periodic ticks for rate limiting
        - url: https://pkg.go.dev/golang.org/x/time/rate
          title: Package rate (x/time)
          note: production-grade rate limiter
      variants:
        - id: v1
          title: Ticker Rate Limiter
          description: >-
            Build a <code>RateLimiter</code> struct that uses <code>time.Ticker</code> to allow at most N operations per
            second. Implement <code>NewRateLimiter(perSecond int) *RateLimiter</code>, <code>Wait()</code> (blocks until
            allowed), and <code>Stop()</code>.
          functionSignature: func NewRateLimiter(perSecond int) *RateLimiter
          testCases:
            - input: NewRateLimiter(5) // 5 ops/sec
              output: Wait() blocks ~200ms between calls
            - input: NewRateLimiter(10) // 10 ops/sec
              output: Wait() blocks ~100ms between calls
          hints:
            - title: ðŸ¤” Think about it
              content: If you want N operations per second, what is the interval between each allowed operation?
            - title: ðŸ’¡ Hint
              content: >-
                Interval is time.Second / time.Duration(perSecond). Create a time.Ticker with that interval. Wait()
                reads from ticker.C.
            - title: ðŸ”§ Pattern
              content: |-
                <pre>1. RateLimiter has a *time.Ticker field
                2. NewRateLimiter calculates interval, creates ticker
                3. Wait() blocks on <-ticker.C
                4. Stop() calls ticker.Stop()</pre>
          solution: |-
            type RateLimiter struct {
                ticker *time.Ticker
            }

            func NewRateLimiter(perSecond int) *RateLimiter {
                return &RateLimiter{
                    ticker: time.NewTicker(time.Second / time.Duration(perSecond)),
                }
            }

            func (r *RateLimiter) Wait() {
                <-r.ticker.C
            }

            func (r *RateLimiter) Stop() {
                r.ticker.Stop()
            }
          difficulty: 3
          annotations:
            - type: stdlib
              label: time.Ticker
              text: >-
                time.NewTicker sends values on its channel at regular intervals. Always call Stop() when done to release
                the ticker's resources.
            - type: idiom
              label: Blocking Channel Read
              text: >-
                <-ticker.C blocks the caller until the next tick. This naturally throttles the calling code without
                busy-waiting.
        - id: v2
          title: Token Bucket Limiter
          description: >-
            Build a <code>TokenBucket</code> rate limiter. It starts with <code>capacity</code> tokens and refills at
            <code>rate</code> tokens per second. <code>Allow()</code> returns true if a token is available (consuming
            it), false otherwise. Use a goroutine for refilling.
          functionSignature: func NewTokenBucket(capacity int, rate int) *TokenBucket
          testCases:
            - input: NewTokenBucket(3, 1) // 3 burst, 1/sec refill
              output: First 3 Allow() return true, 4th returns false
            - input: NewTokenBucket(1, 10) // 1 burst, 10/sec refill
              output: Allow() returns true, then false, then true after 100ms
          hints:
            - title: ðŸ¤” Think about it
              content: How do you safely modify the token count from both the refill goroutine and the Allow method?
            - title: ðŸ’¡ Hint
              content: >-
                Use a mutex to protect the tokens field. A background goroutine adds tokens at the refill rate using a
                ticker. Allow() locks, checks tokens > 0, decrements, and returns.
            - title: ðŸ”§ Pattern
              content: |-
                <pre>1. TokenBucket: tokens, capacity int, mu sync.Mutex, stop chan
                2. Background goroutine: ticker at rate, add token if below capacity
                3. Allow(): lock, if tokens > 0 decrement and return true
                4. Stop(): close stop channel, stop ticker</pre>
          solution: |-
            type TokenBucket struct {
                tokens   int
                capacity int
                mu       sync.Mutex
                stop     chan struct{}
                ticker   *time.Ticker
            }

            func NewTokenBucket(capacity int, rate int) *TokenBucket {
                tb := &TokenBucket{
                    tokens:   capacity,
                    capacity: capacity,
                    stop:     make(chan struct{}),
                    ticker:   time.NewTicker(time.Second / time.Duration(rate)),
                }
                go func() {
                    for {
                        select {
                        case <-tb.ticker.C:
                            tb.mu.Lock()
                            if tb.tokens < tb.capacity {
                                tb.tokens++
                            }
                            tb.mu.Unlock()
                        case <-tb.stop:
                            tb.ticker.Stop()
                            return
                        }
                    }
                }()
                return tb
            }

            func (tb *TokenBucket) Allow() bool {
                tb.mu.Lock()
                defer tb.mu.Unlock()
                if tb.tokens > 0 {
                    tb.tokens--
                    return true
                }
                return false
            }

            func (tb *TokenBucket) Stop() {
                close(tb.stop)
            }
          difficulty: 3
          annotations:
            - type: pattern
              label: Token Bucket
              text: >-
                The token bucket algorithm allows bursts up to capacity while maintaining an average rate. It is widely
                used in API rate limiting and network traffic shaping.
            - type: gotcha
              label: Mutex + Channel
              text: >-
                The mutex protects the tokens field from concurrent access. The stop channel signals the background
                goroutine to exit. Mixing both is fine when they protect different things.
        - id: v3
          title: Sliding Window Limiter
          description: >-
            Build a <code>SlidingWindow</code> rate limiter that allows at most <code>maxRequests</code> in the last
            <code>window</code> duration. <code>Allow()</code> returns true if under the limit. Track request timestamps
            and evict expired ones.
          functionSignature: func NewSlidingWindow(maxRequests int, window time.Duration) *SlidingWindow
          testCases:
            - input: NewSlidingWindow(3, time.Second)
              output: 3 rapid Allow() return true, 4th returns false, true again after 1s
            - input: NewSlidingWindow(1, 500*time.Millisecond)
              output: Allow() true, then false, then true after 500ms
          hints:
            - title: ðŸ¤” Think about it
              content: How do you track when each request happened? How do you efficiently remove old entries?
            - title: ðŸ’¡ Hint
              content: >-
                Store timestamps in a slice. On each Allow(), remove timestamps older than now-window. If
                len(timestamps) < maxRequests, add now and return true.
            - title: ðŸ”§ Pattern
              content: |-
                <pre>1. SlidingWindow: timestamps []time.Time, max, window, mu
                2. Allow(): lock, evict old timestamps
                3. If len < max: append now, return true
                4. Else return false</pre>
          solution: |-
            type SlidingWindow struct {
                timestamps  []time.Time
                maxRequests int
                window      time.Duration
                mu          sync.Mutex
            }

            func NewSlidingWindow(maxRequests int, window time.Duration) *SlidingWindow {
                return &SlidingWindow{
                    maxRequests: maxRequests,
                    window:      window,
                }
            }

            func (sw *SlidingWindow) Allow() bool {
                sw.mu.Lock()
                defer sw.mu.Unlock()

                now := time.Now()
                cutoff := now.Add(-sw.window)

                // Evict expired timestamps
                valid := sw.timestamps[:0]
                for _, ts := range sw.timestamps {
                    if ts.After(cutoff) {
                        valid = append(valid, ts)
                    }
                }
                sw.timestamps = valid

                if len(sw.timestamps) < sw.maxRequests {
                    sw.timestamps = append(sw.timestamps, now)
                    return true
                }
                return false
            }
          difficulty: 3
          annotations:
            - type: pattern
              label: Sliding Window
              text: >-
                The sliding window tracks individual request times, giving more accurate rate limiting than fixed
                windows. It prevents burst-at-boundary attacks.
            - type: complexity
              label: O(n)
              text: >-
                Each Allow() scans all stored timestamps to evict expired ones. For high-throughput systems, consider a
                ring buffer or sorted structure for O(1) eviction.
        - id: v4
          title: Per-Key Rate Limiter
          description: >-
            Build a <code>PerKeyLimiter</code> that rate-limits per key (e.g., per user or per IP). <code>Allow(key
            string)</code> returns true if the key has not exceeded <code>maxPerSecond</code> requests. Use a map of
            last-allowed timestamps.
          functionSignature: func NewPerKeyLimiter(maxPerSecond int) *PerKeyLimiter
          testCases:
            - input: limiter.Allow("user1") // first call
              output: "true"
            - input: limiter.Allow("user1") // immediate second call, maxPerSecond=1
              output: "false"
            - input: limiter.Allow("user2") // different key
              output: "true"
          hints:
            - title: ðŸ¤” Think about it
              content: How do you track rate limits independently for each key? What data structure maps keys to their state?
            - title: ðŸ’¡ Hint
              content: >-
                Use a map[string]time.Time to store the last-allowed time per key. If now - lastAllowed >= interval,
                allow and update. Use a mutex for thread safety.
            - title: ðŸ”§ Pattern
              content: |-
                <pre>1. PerKeyLimiter: limits map[string]time.Time, interval, mu
                2. interval = time.Second / maxPerSecond
                3. Allow(key): lock, check last time for key
                4. If enough time passed or new key: update and return true</pre>
          solution: |-
            type PerKeyLimiter struct {
                limits   map[string]time.Time
                interval time.Duration
                mu       sync.Mutex
            }

            func NewPerKeyLimiter(maxPerSecond int) *PerKeyLimiter {
                return &PerKeyLimiter{
                    limits:   make(map[string]time.Time),
                    interval: time.Second / time.Duration(maxPerSecond),
                }
            }

            func (l *PerKeyLimiter) Allow(key string) bool {
                l.mu.Lock()
                defer l.mu.Unlock()

                now := time.Now()
                last, exists := l.limits[key]
                if !exists || now.Sub(last) >= l.interval {
                    l.limits[key] = now
                    return true
                }
                return false
            }
          difficulty: 3
          annotations:
            - type: pattern
              label: Per-Key Limiting
              text: >-
                Per-key rate limiting is essential for multi-tenant systems. Each user or IP gets its own rate limit,
                preventing one user from starving others.
            - type: gotcha
              label: Memory Growth
              text: >-
                The limits map grows unboundedly. In production, add a background cleanup goroutine that evicts entries
                older than the window to prevent memory leaks.
    - id: challenge_3
      block: 3
      difficulty: 4
      concept: Graceful Shutdown
      docLinks:
        - url: https://pkg.go.dev/os/signal#Notify
          title: Package signal.Notify
          note: receiving OS signals
        - url: https://pkg.go.dev/context#WithCancel
          title: Package context.WithCancel
          note: cancellation propagation
        - url: https://pkg.go.dev/net/http#Server.Shutdown
          title: Package http.Server.Shutdown
          note: graceful HTTP shutdown
      variants:
        - id: v1
          title: Signal-Based Shutdown
          description: >-
            Write a program that starts a worker goroutine printing "working..." every 100ms. On receiving
            <code>os.Interrupt</code> (Ctrl+C), signal the worker to stop via a channel, wait for it to finish, then
            print "Done" and exit.
          functionSignature: func main()
          testCases:
            - input: Run program, press Ctrl+C after ~500ms
              output: Several 'working...' lines, then 'Worker stopping...', 'Done'
            - input: Run program, press Ctrl+C immediately
              output: "'Worker stopping...', 'Done'"
          hints:
            - title: ðŸ¤” Think about it
              content: How does the main goroutine tell the worker to stop? How does it know the worker has actually finished?
            - title: ðŸ’¡ Hint
              content: >-
                Use two channels: 'stop' (main tells worker to stop) and 'done' (worker confirms it stopped). The worker
                uses select to check for stop signals between work iterations.
            - title: ðŸ”§ Pattern
              content: |-
                <pre>1. Create stop and done channels
                2. Worker: select on stop or default (do work)
                3. signal.Notify(sigCh, os.Interrupt)
                4. <-sigCh, close(stop), <-done, print Done</pre>
          solution: |-
            func main() {
                stop := make(chan struct{})
                done := make(chan struct{})

                go func() {
                    defer close(done)
                    for {
                        select {
                        case <-stop:
                            fmt.Println("Worker stopping...")
                            return
                        default:
                            fmt.Println("working...")
                            time.Sleep(100 * time.Millisecond)
                        }
                    }
                }()

                sigCh := make(chan os.Signal, 1)
                signal.Notify(sigCh, os.Interrupt)
                <-sigCh

                fmt.Println("Shutting down...")
                close(stop)
                <-done
                fmt.Println("Done")
            }
          difficulty: 4
          annotations:
            - type: idiom
              label: Close to Broadcast
              text: >-
                Closing a channel wakes all goroutines blocked on it. This is the Go way to broadcast a cancellation
                signal to multiple workers.
            - type: stdlib
              label: os/signal
              text: >-
                signal.Notify registers for OS signals. Use a buffered channel (size 1) to avoid missing the signal if
                the receiver is not ready.
        - id: v2
          title: Context-Based Shutdown
          description: >-
            Rewrite graceful shutdown using <code>context.Context</code>. Create a cancellable context, pass it to the
            worker. On Ctrl+C, cancel the context. The worker checks <code>ctx.Done()</code> in its select loop.
          functionSignature: func worker(ctx context.Context, done chan struct{})
          testCases:
            - input: Run program, press Ctrl+C after ~300ms
              output: Several 'working...' lines, then 'context canceled', 'Done'
            - input: Run program, press Ctrl+C immediately
              output: "'context canceled', 'Done'"
          hints:
            - title: ðŸ¤” Think about it
              content: How does context.WithCancel propagate cancellation? What channel does the worker monitor?
            - title: ðŸ’¡ Hint
              content: >-
                Create ctx, cancel := context.WithCancel(context.Background()). Pass ctx to the worker. Worker selects
                on ctx.Done(). Main calls cancel() on signal.
            - title: ðŸ”§ Pattern
              content: |-
                <pre>1. ctx, cancel := context.WithCancel(context.Background())
                2. Worker: select on ctx.Done() or default
                3. On signal: cancel()
                4. <-done to wait for worker</pre>
          solution: |-
            func worker(ctx context.Context, done chan struct{}) {
                defer close(done)
                for {
                    select {
                    case <-ctx.Done():
                        fmt.Println(ctx.Err())
                        return
                    default:
                        fmt.Println("working...")
                        time.Sleep(100 * time.Millisecond)
                    }
                }
            }

            func main() {
                ctx, cancel := context.WithCancel(context.Background())
                done := make(chan struct{})

                go worker(ctx, done)

                sigCh := make(chan os.Signal, 1)
                signal.Notify(sigCh, os.Interrupt)
                <-sigCh

                fmt.Println("Shutting down...")
                cancel()
                <-done
                fmt.Println("Done")
            }
          difficulty: 4
          annotations:
            - type: idiom
              label: Context Cancellation
              text: >-
                Using context for cancellation is idiomatic Go. It integrates with the standard library (net/http,
                database/sql) and propagates through call chains.
            - type: alternative
              label: signal.NotifyContext
              text: >-
                Go 1.16+ provides signal.NotifyContext which combines signal handling and context cancellation in one
                call, simplifying the pattern.
        - id: v3
          title: Multi-Worker Shutdown
          description: >-
            Start 3 worker goroutines, each with an ID. On Ctrl+C, signal all workers to stop, wait for all to finish
            using a <code>sync.WaitGroup</code>, then exit. Each worker should print its ID when stopping.
          functionSignature: func main()
          testCases:
            - input: Run program, press Ctrl+C
              output: All 3 workers print stopping messages, then 'All workers done'
            - input: Workers have IDs 1, 2, 3
              output: Each worker prints 'Worker N stopping...'
          hints:
            - title: ðŸ¤” Think about it
              content: How do you signal all workers at once? How do you wait for all of them to finish?
            - title: ðŸ’¡ Hint
              content: >-
                Close a shared 'stop' channel to broadcast to all workers. Use sync.WaitGroup with Add(3), and each
                worker calls Done() when finished.
            - title: ðŸ”§ Pattern
              content: |-
                <pre>1. stop channel, WaitGroup
                2. Launch 3 workers: wg.Add(1) each, defer wg.Done()
                3. Workers select on stop
                4. On signal: close(stop), wg.Wait()
                5. Print 'All workers done'</pre>
          solution: |-
            func main() {
                stop := make(chan struct{})
                var wg sync.WaitGroup

                for i := 1; i <= 3; i++ {
                    wg.Add(1)
                    go func(id int) {
                        defer wg.Done()
                        for {
                            select {
                            case <-stop:
                                fmt.Printf("Worker %d stopping...\n", id)
                                return
                            default:
                                fmt.Printf("Worker %d working...\n", id)
                                time.Sleep(100 * time.Millisecond)
                            }
                        }
                    }(i)
                }

                sigCh := make(chan os.Signal, 1)
                signal.Notify(sigCh, os.Interrupt)
                <-sigCh

                fmt.Println("Shutting down...")
                close(stop)
                wg.Wait()
                fmt.Println("All workers done")
            }
          difficulty: 4
          annotations:
            - type: idiom
              label: WaitGroup for Completion
              text: >-
                sync.WaitGroup tracks N goroutines. Each calls Done() when finished. Wait() blocks until all are done.
                This is simpler than counting 'done' channels.
            - type: gotcha
              label: Loop Variable Capture
              text: >-
                Pass the loop variable 'i' as a function argument: go func(id int){...}(i). Capturing 'i' directly in
                the closure would give all goroutines the same value.
        - id: v4
          title: Shutdown with Drain
          description: >-
            Start a worker that processes jobs from a channel. On Ctrl+C, stop sending new jobs but let the worker
            finish all queued jobs before exiting. Print the count of processed jobs at the end.
          functionSignature: func main()
          testCases:
            - input: Queue 10 jobs, Ctrl+C after 3 processed
              output: All 10 jobs processed, then 'Processed 10 jobs'
            - input: Queue 5 jobs, Ctrl+C before any processed
              output: All 5 jobs processed, then 'Processed 5 jobs'
          hints:
            - title: ðŸ¤” Think about it
              content: >-
                How does the worker know there are no more jobs coming? What is the difference between stopping
                immediately and draining?
            - title: ðŸ’¡ Hint
              content: >-
                Close the jobs channel to signal no more jobs. The worker ranges over the channel and processes all
                remaining items. This naturally drains the queue.
            - title: ðŸ”§ Pattern
              content: |-
                <pre>1. Buffered jobs channel, done channel
                2. Producer goroutine sends jobs (checks for stop)
                3. Worker ranges over jobs channel
                4. On signal: close(stop), producer closes jobs
                5. <-done waits for worker to drain all jobs</pre>
          solution: |-
            func main() {
                jobs := make(chan int, 20)
                done := make(chan int)
                stop := make(chan struct{})

                // Producer
                go func() {
                    i := 0
                    for {
                        select {
                        case <-stop:
                            close(jobs)
                            return
                        default:
                            i++
                            jobs <- i
                            time.Sleep(50 * time.Millisecond)
                        }
                    }
                }()

                // Worker
                go func() {
                    count := 0
                    for job := range jobs {
                        fmt.Printf("Processing job %d\n", job)
                        time.Sleep(100 * time.Millisecond)
                        count++
                    }
                    done <- count
                }()

                sigCh := make(chan os.Signal, 1)
                signal.Notify(sigCh, os.Interrupt)
                <-sigCh

                fmt.Println("Shutting down, draining queue...")
                close(stop)
                count := <-done
                fmt.Printf("Processed %d jobs\n", count)
            }
          difficulty: 4
          annotations:
            - type: pattern
              label: Graceful Drain
              text: >-
                Draining a queue before shutdown ensures no work is lost. This is critical for message queues, task
                processors, and database write buffers.
            - type: idiom
              label: Range Over Channel
              text: >-
                Using range on the jobs channel processes all buffered items until the channel is closed. This is the
                cleanest way to drain a work queue in Go.
            - type: gotcha
              label: Buffered Channel Drain
              text: >-
                A buffered channel can hold items even after close(). The worker's range loop will process all buffered
                items before ending. This is by design.
